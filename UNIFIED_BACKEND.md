# Unified Backend - Express + Python ML

## Architecture

The backend is now a **unified service** that runs:

1. **Express API** (Node.js) - Main API server (Port 3000)
2. **Python ML Service** (Flask) - ML workloads (Port 8000)

Both run in the same process/container and communicate internally.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Unified Backend Service              â”‚
â”‚                                        â”‚
â”‚   Express (Port 3000)                  â”‚
â”‚   â”œâ”€â”€ /api/users                       â”‚
â”‚   â”œâ”€â”€ /api/auth                        â”‚
â”‚   â”œâ”€â”€ /api/curriculum                  â”‚
â”‚   â””â”€â”€ /api/ml/* â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚                       â”‚                â”‚
â”‚                       â–¼                â”‚
â”‚   Python ML (Port 8000)                â”‚
â”‚   â””â”€â”€ /signphony/*                     â”‚
â”‚       â”œâ”€â”€ /signs                       â”‚
â”‚       â”œâ”€â”€ /magic-tricks                â”‚
â”‚       â””â”€â”€ /translate                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Directory Structure

```
api/
â”œâ”€â”€ server.js              # Express main server
â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ auth.js
â”‚   â”œâ”€â”€ users.js
â”‚   â”œâ”€â”€ ml.js             # NEW: ML proxy routes
â”‚   â””â”€â”€ ...
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ ml-service.js     # NEW: Python subprocess manager
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ml/                    # NEW: Python ML service
â”‚   â”œâ”€â”€ app.py            # Flask entry point
â”‚   â”œâ”€â”€ requirements.txt  # Python dependencies
â”‚   â””â”€â”€ signphony/        # Signphony backend
â”‚       â”œâ”€â”€ database.py
â”‚       â”œâ”€â”€ magic_tricks.py
â”‚       â”œâ”€â”€ unified_api.py
â”‚       â”œâ”€â”€ shared/
â”‚       â””â”€â”€ translator/
â””â”€â”€ package.json
```

## How It Works

### 1. Server Startup

```javascript
// server.js starts Express
const app = express();

// Express spawns Python ML service
await mlService.start();
// â†’ Spawns: python3 ml/app.py

// Express registers ML proxy routes
app.use('/api/ml', mlRoutes);
```

### 2. Request Flow

```
Frontend â†’ Express â†’ Python â†’ Response

Example:
GET /api/ml/signphony/signs
  â†’ Express receives request
  â†’ Proxies to http://localhost:8000/signphony/signs
  â†’ Python Flask handles it
  â†’ Express returns response to frontend
```

### 3. Process Management

- Express spawns Python as a **child process**
- If Python crashes, Express can restart it
- Graceful shutdown stops both Express and Python
- Health checks monitor both services

## Development

### Install Dependencies

```bash
cd /Volumes/ll-ssd/projects/lit/lit-mvp/api

# Node dependencies
npm install

# Python dependencies
cd ml
pip3 install -r requirements.txt
cd ..
```

### Run Unified Backend

```bash
# Start both Express + Python
npm run dev

# You'll see:
# âœ“ Server running on port 3000
# [ML Service] Starting Python ML service...
# [ML Service] Port: 8000
# âœ“ ML service started successfully
```

### Test Endpoints

```bash
# Express health
curl http://localhost:3000/health

# ML health (through Express proxy)
curl http://localhost:3000/api/ml/health

# Signphony signs
curl http://localhost:3000/api/ml/signphony/signs

# Direct ML (for debugging)
curl http://localhost:8000/health
```

## Frontend Integration

### Old Way (Separate Deployments)

```javascript
// Frontend called two different APIs
const mainApi = 'http://localhost:3000';
const signphonyApi = 'http://localhost:8000';

await fetch(`${mainApi}/users`);
await fetch(`${signphonyApi}/signphony/signs`);
```

### New Way (Unified Backend)

```javascript
// Frontend calls ONE API for everything
const api = 'http://localhost:3000';

await fetch(`${api}/api/users`);
await fetch(`${api}/api/ml/signphony/signs`);
```

Update frontend `.env`:

```env
VITE_API_URL=http://localhost:3000
```

## Deployment

### Railway

Deploy as a single service:

```bash
cd /Volumes/ll-ssd/projects/lit/lit-mvp
railway up

# Railway will:
# 1. Build Docker image with Node + Python
# 2. Start Express server
# 3. Express spawns Python ML service
# 4. Both run in same container
```

### Dockerfile

```dockerfile
FROM node:20-slim

# Install Python
RUN apt-get install -y python3 python3-pip

# Install Node deps
COPY package*.json ./
RUN npm ci

# Install Python deps
COPY ml/requirements.txt ./ml/
RUN pip3 install -r ml/requirements.txt

# Copy code
COPY . .

# Start Express (which spawns Python)
CMD ["node", "server.js"]
```

## Environment Variables

```bash
# Node.js
PORT=3000
NODE_ENV=production
JWT_SECRET=your-secret

# Python ML
ML_PORT=8000
ML_HOST=localhost
PYTHON_CMD=python3

# Database (shared by both)
DATABASE_URL=postgres://...
```

## Benefits

âœ… **Single Deployment** - One Railway service instead of two
âœ… **Shared Database** - No data sync issues
âœ… **Unified Auth** - Express handles auth for all endpoints
âœ… **Cost Efficient** - One server, not two
âœ… **Easier Development** - Start with one command
âœ… **Better Monitoring** - All logs in one place

## Monitoring

### Health Checks

```bash
# Overall health
curl http://localhost:3000/health

# ML service health
curl http://localhost:3000/api/ml/health
```

### Logs

All logs are unified:

```
[Express] Server running on port 3000
[ML Service] Starting Python ML service...
[ML Service] Port: 8000
[ML Service] âœ“ Database initialized
[Express] âœ“ ML service started successfully
```

## Troubleshooting

### Python Service Won't Start

```bash
# Check Python is installed
python3 --version

# Check dependencies
cd ml
pip3 install -r requirements.txt

# Run Python directly (debug)
python3 app.py
```

### Proxy Errors

```bash
# Check ML service is running
curl http://localhost:8000/health

# Check proxy routes
curl http://localhost:3000/api/ml/health
```

### Port Conflicts

```bash
# If ports are in use, change them
export PORT=3001
export ML_PORT=8001
npm run dev
```

## Production Checklist

- [ ] Install both Node and Python dependencies
- [ ] Set environment variables
- [ ] Configure DATABASE_URL for both services
- [ ] Enable health check monitoring
- [ ] Set up log aggregation
- [ ] Configure restart policies
- [ ] Test ML service failover

## File Changes Summary

### New Files Created

- `ml/app.py` - Python ML service entry point
- `ml/requirements.txt` - Python dependencies
- `ml/signphony/` - Signphony backend code (moved)
- `services/ml-service.js` - Python subprocess manager
- `routes/ml.js` - ML proxy routes

### Files to Update

- `server.js` - Add ML service startup and routes (see SERVER_UPDATES.md)
- `package.json` - No changes needed (already has dependencies)

### Migration

1. âœ… Python code moved to `api/ml/signphony/`
2. âœ… ML service entry point created (`ml/app.py`)
3. âœ… Subprocess manager created (`services/ml-service.js`)
4. âœ… Proxy routes created (`routes/ml.js`)
5. â³ Update `server.js` (see SERVER_UPDATES.md)
6. â³ Test integration
7. â³ Deploy to Railway

## Next Steps

1. Apply updates from `SERVER_UPDATES.md` to `server.js`
2. Install Python dependencies: `cd ml && pip3 install -r requirements.txt`
3. Test locally: `npm run dev`
4. Update frontend to use unified API
5. Deploy to Railway

You now have a **unified backend** that's easier to develop, deploy, and maintain! ğŸ‰
